AWSTemplateFormatVersion: '2010-09-09'
Description: 部署用于 EC2 指标分析的 Lambda 函数和 Step Functions，并将结果输出到 S3

Parameters:
  RetryInterval:
    Type: Number
    Default: 2
    Description: "重试之间的时间间隔（秒）。"
  MaxRetryAttempts:
    Type: Number
    Default: 3
    Description: "最大重试次数。"
  BackoffRate:
    Type: Number
    Default: 2.0
    Description: "指数退避的退避率。"
  LogGroupName:
    Type: String
    Default: "/aws/vendedlogs/StepFunctions/MyStateMachine"
    Description: "Step Functions 日志组的名称。"
  S3BucketName:
    Type: String
    Default: "my-s3-bucket"
    Description: "存储结果的 S3 桶名称。"
  S3Path:
    Type: String
    Default: "ABC"
    Description: "存储结果的 S3 路径。"
  MetricName:
    Type: String
    Default: "ExecutionFailed"
    Description: "要监控的 CloudWatch 指标名称。"
  SnsTopicArn:
    Type: String
    Description: "用于警报通知的 SNS 主题 ARN。"
  ScheduleExpression:
    Type: String
    Default: "cron(0 0 1 * ? *)"
    Description: "EventBridge 规则的调度表达式。"


Resources:
  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Ref LogGroupName
      RetentionInDays: 14

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaBasicExecution
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:HeadBucket
                Resource: !Sub "arn:aws:s3:::${S3BucketName}/${S3Path}/*"

  CpuAnalysisFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.12
      Timeout: 900
      Architectures:
        - x86_64
      MemorySize: 256     
      EphemeralStorage:
        Size: 512
      Environment:
        Variables:
          S3_BUCKET_NAME: !Ref S3BucketName
          S3_PATH: !Ref S3Path
          A: a
          B: b
          C: c
      Code:
        ZipFile: |
          import os
          import boto3
          import csv
          from datetime import datetime
          from botocore.exceptions import ClientError

          def lambda_handler(event, context):
              s3 = boto3.client('s3')
              bucket_name = os.environ['S3_BUCKET_NAME']
              s3_path = os.environ['S3_PATH']

              try:
                  s3.head_bucket(Bucket=bucket_name)
              except ClientError as e:
                  if e.response['Error']['Code'] == '404':
                      return {
                          'statusCode': 404,
                          'body': f'S3 bucket {bucket_name} does not exist.'
                      }
                  else:
                      return {
                          'statusCode': 500,
                          'body': 'Error checking S3 bucket.'
                      }
              
              cpu_data = {"CPU Analysis": "Completed"}

              csv_file = '/tmp/cpu_analysis.csv'
              with open(csv_file, mode='w', newline='') as file:
                  writer = csv.writer(file)
                  writer.writerow(cpu_data.keys())
                  writer.writerow(cpu_data.values())
              
              current_date = datetime.now().strftime('%Y-%m-%d')
              s3_key = f'{s3_path}/cpu_analysis_{current_date}.csv'

              try:
                  s3.upload_file(csv_file, bucket_name, s3_key)
              except ClientError as e:
                  return {
                      'statusCode': 500,
                      'body': f'Error uploading to S3: {e}'
                  }

              return {
                  'statusCode': 200,
                  'body': 'CPU analysis completed and uploaded to S3'
              }

  MemoryAnalysisFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.12
      Timeout: 900
      Architectures:
        - x86_64
      MemorySize: 256     
      EphemeralStorage:
        Size: 512
      Environment:
        Variables:
          S3_BUCKET_NAME: !Ref S3BucketName
          S3_PATH: !Ref S3Path
          A: a
          B: b
          C: c
      Code:
        ZipFile: |
          import os
          import boto3
          import csv
          from datetime import datetime
          from botocore.exceptions import ClientError

          def lambda_handler(event, context):
              s3 = boto3.client('s3')
              bucket_name = os.environ['S3_BUCKET_NAME']
              s3_path = os.environ['S3_PATH']

              try:
                  s3.head_bucket(Bucket=bucket_name)
              except ClientError as e:
                  if e.response['Error']['Code'] == '404':
                      return {
                          'statusCode': 404,
                          'body': f'S3 bucket {bucket_name} does not exist.'
                      }
                  else:
                      return {
                          'statusCode': 500,
                          'body': 'Error checking S3 bucket.'
                      }

              memory_data = {"Memory Analysis": "Completed"}

              csv_file = '/tmp/memory_analysis.csv'
              with open(csv_file, mode='w', newline='') as file:
                  writer = csv.writer(file)
                  writer.writerow(memory_data.keys())
                  writer.writerow(memory_data.values())
              
              current_date = datetime.now().strftime('%Y-%m-%d')
              s3_key = f'{s3_path}/memory_analysis_{current_date}.csv'

              try:
                  s3.upload_file(csv_file, bucket_name, s3_key)
              except ClientError as e:
                  return {
                      'statusCode': 500,
                      'body': f'Error uploading to S3: {e}'
                  }

              return {
                  'statusCode': 200,
                  'body': 'Memory analysis completed and uploaded to S3'
              }

  DiskSpaceAnalysisFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.12
      Timeout: 900
      Architectures:
        - x86_64
      MemorySize: 256     
      EphemeralStorage:
        Size: 512
      Environment:
        Variables:
          S3_BUCKET_NAME: !Ref S3BucketName
          S3_PATH: !Ref S3Path
          A: a
          B: b
          C: c
      Code:
        ZipFile: |
          import os
          import boto3
          import csv
          from datetime import datetime
          from botocore.exceptions import ClientError

          def lambda_handler(event, context):
              s3 = boto3.client('s3')
              bucket_name = os.environ['S3_BUCKET_NAME']
              s3_path = os.environ['S3_PATH']

              try:
                  s3.head_bucket(Bucket=bucket_name)
              except ClientError as e:
                  if e.response['Error']['Code'] == '404':
                      return {
                          'statusCode': 404,
                          'body': f'S3 bucket {bucket_name} does not exist.'
                      }
                  else:
                      return {
                          'statusCode': 500,
                          'body': 'Error checking S3 bucket.'
                      }

              disk_data = {"Disk Space Analysis": "Completed"}

              csv_file = '/tmp/disk_analysis.csv'
              with open(csv_file, mode='w', newline='') as file:
                  writer = csv.writer(file)
                  writer.writerow(disk_data.keys())
                  writer.writerow(disk_data.values())
              
              current_date = datetime.now().strftime('%Y-%m-%d')
              s3_key = f'{s3_path}/disk_analysis_{current_date}.csv'

              try:
                  s3.upload_file(csv_file, bucket_name, s3_key)
              except ClientError as e:
                  return {
                      'statusCode': 500,
                      'body': f'Error uploading to S3: {e}'
                  }

              return {
                  'statusCode': 200,
                  'body': 'Disk space analysis completed and uploaded to S3'
              }

  StateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      DefinitionString:
        Fn::Sub:
          - |
            {
              "Comment": "A state machine to analyze EC2 metrics",
              "StartAt": "CpuAnalysis",
              "States": {
                "CpuAnalysis": {
                  "Type": "Task",
                  "Resource": "${CpuAnalysisFunction.Arn}",
                  "Next": "MemoryAnalysis",
                  "Retry": [
                    {
                      "ErrorEquals": ["States.ALL"],
                      "IntervalSeconds": "${RetryInterval}",
                      "MaxAttempts": ${MaxRetryAttempts},
                      "BackoffRate": ${BackoffRate}
                    }
                  ],
                  "Catch": [
                    {
                      "ErrorEquals": ["States.ALL"],
                      "ResultPath": "$.error-info",
                      "Next": "ErrorHandler"
                    }
                  ],
                  "ResultPath": "$.CpuAnalysisResult"
                },
                "MemoryAnalysis": {
                  "Type": "Task",
                  "Resource": "${MemoryAnalysisFunction.Arn}",
                  "Next": "DiskSpaceAnalysis",
                  "Retry": [
                    {
                      "ErrorEquals": ["States.ALL"],
                      "IntervalSeconds": "${RetryInterval}",
                      "MaxAttempts": ${MaxRetryAttempts},
                      "BackoffRate": ${BackoffRate}
                    }
                  ],
                  "Catch": [
                    {
                      "ErrorEquals": ["States.ALL"],
                      "ResultPath": "$.error-info",
                      "Next": "ErrorHandler"
                    }
                  ],
                  "ResultPath": "$.MemoryAnalysisResult"
                },
                "DiskSpaceAnalysis": {
                  "Type": "Task",
                  "Resource": "${DiskSpaceAnalysisFunction.Arn}",
                  "End": true,
                  "Retry": [
                    {
                      "ErrorEquals": ["States.ALL"],
                      "IntervalSeconds": "${RetryInterval}",
                      "MaxAttempts": ${MaxRetryAttempts},
                      "BackoffRate": ${BackoffRate}
                    }
                  ],
                  "Catch": [
                    {
                      "ErrorEquals": ["States.ALL"],
                      "ResultPath": "$.error-info",
                      "Next": "ErrorHandler"
                    }
                  ],
                  "ResultPath": "$.DiskSpaceAnalysisResult"
                },
                "ErrorHandler": {
                  "Type": "Pass",
                  "ResultPath": "$.error-info",
                  "Result": {
                    "ErrorMessage": "An error occurred during execution.",
                    "ErrorDetails.$": "$.error-info"
                  },
                  "End": true
                }
              }
            }
          - CpuAnalysisFunction: !GetAtt CpuAnalysisFunction.Arn
            MemoryAnalysisFunction: !GetAtt MemoryAnalysisFunction.Arn
            DiskSpaceAnalysisFunction: !GetAtt DiskSpaceAnalysisFunction.Arn
      LoggingConfiguration:
        Destinations:
          - CloudWatchLogsLogGroup:
              LogGroupArn: !GetAtt LogGroup.Arn
        IncludeExecutionData: true
        Level: ALL

  StateMachineFailureAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmDescription: "Alarm for Step Function execution failures"
      MetricName: !Ref MetricName
      Namespace: "AWS/States"
      Statistic: "Sum"
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: "GreaterThanOrEqualToThreshold"
      Dimensions:
        - Name: "StateMachineArn"
          Value: !Ref StateMachine
      AlarmActions:
        - !Ref SnsTopicArn
      OKActions:
        - !Ref SnsTopicArn
      InsufficientDataActions:
        - !Ref SnsTopicArn

  MonthlyTriggerRule:
    Type: AWS::Events::Rule
    Properties:
      ScheduleExpression: !Ref ScheduleExpression  # 使用传入的参数
      Targets:
        - Arn: !GetAtt StateMachine.Arn  # 目标为状态机的 ARN
          Id: "StateMachineTarget"  # 目标 ID
          RoleArn: !GetAtt LambdaExecutionRole.Arn  # 授予 EventBridge 触发状态机的权限

  EventBridgeInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: "states:StartExecution"  # 允许的操作
      FunctionName: !GetAtt StateMachine.Arn  # 目标函数为状态机
      Principal: "events.amazonaws.com"  # 事件桥的服务主体
      SourceArn: !GetAtt MonthlyTriggerRule.Arn  # 源 ARN

Outputs:
  StateMachineArn:
    Description: "ARN of the State Machine"
    Value: !Ref StateMachine

  LogGroupArn:
    Description: "ARN of the CloudWatch Log Group"
    Value: !GetAtt LogGroup.Arn

  LambdaExecutionRoleArn:
    Description: "ARN of the Lambda Execution Role"
    Value: !GetAtt LambdaExecutionRole.Arn

  CpuAnalysisFunctionArn:
    Description: "ARN of the CPU Analysis Lambda Function"
    Value: !GetAtt CpuAnalysisFunction.Arn

  MemoryAnalysisFunctionArn:
    Description: "ARN of the Memory Analysis Lambda Function"
    Value: !GetAtt MemoryAnalysisFunction.Arn

  DiskSpaceAnalysisFunctionArn:
    Description: "ARN of the Disk Space Analysis Lambda Function"
    Value: !GetAtt DiskSpaceAnalysisFunction.Arn

  StateMachineFailureAlarmArn:
    Description: "ARN of the State Machine Failure Alarm"
    Value: !Ref StateMachineFailureAlarm

  MonthlyTriggerRuleArn:
    Description: "ARN of the EventBridge Monthly Trigger Rule"
    Value: !Ref MonthlyTriggerRule
